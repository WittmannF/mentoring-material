{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Word2Vec from Scratch - Solved.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z8LiEQKEANHg",
        "wz1bfXvTANHk",
        "SmW0RtnGANHz",
        "4Jkr_CapANH5"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT8_02oBANGd",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec\n",
        "- **Source:** https://www.youtube.com/watch?v=64qSgA66P-8\n",
        "\n",
        "Here I implement word2vec with very simple example using tensorflow  \n",
        "word2vec is vector representation for words with similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSI0ih5-ANGe",
        "colab_type": "text"
      },
      "source": [
        "# Collect Data\n",
        "we will use only 10 sentences to create word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMakFQn8ANGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = ['king is a strong man', \n",
        "          'queen is a wise woman', \n",
        "          'boy is a young man',\n",
        "          'girl is a young woman',\n",
        "          'prince is a young king',\n",
        "          'princess is a young queen',\n",
        "          'man is strong', \n",
        "          'woman is pretty',\n",
        "          'prince is a boy will be king',\n",
        "          'princess is a girl will be queen']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU9zNlhXsDO-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5dcfb30-9bd7-4e3f-fbb8-89ade20793c3"
      },
      "source": [
        "len(corpus)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvINrjsgANGr",
        "colab_type": "text"
      },
      "source": [
        "# Remove stop words\n",
        "In order for efficiency of creating word vector, we will remove commonly used words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byogOePoANGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stop_words(corpus):\n",
        "    stop_words = ['is', 'a', 'will', 'be']\n",
        "    results = []\n",
        "    for text in corpus:\n",
        "        tmp = text.split(' ')\n",
        "        for stop_word in stop_words:\n",
        "            if stop_word in tmp:\n",
        "                tmp.remove(stop_word)\n",
        "        results.append(\" \".join(tmp))\n",
        "    \n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0LeU3vsANGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = remove_stop_words(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktYdQAXKAYpj",
        "colab_type": "code",
        "outputId": "c888a074-394a-4279-836a-342214628587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "corpus"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['king strong man',\n",
              " 'queen wise woman',\n",
              " 'boy young man',\n",
              " 'girl young woman',\n",
              " 'prince young king',\n",
              " 'princess young queen',\n",
              " 'man strong',\n",
              " 'woman pretty',\n",
              " 'prince boy king',\n",
              " 'princess girl queen']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B6acqx0ANG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "for text in corpus:\n",
        "    for word in text.split(' '):\n",
        "        words.append(word)\n",
        "\n",
        "words = set(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_JBDsMANG7",
        "colab_type": "text"
      },
      "source": [
        "here we have word set by which we will have word vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNt1PvNOANG8",
        "colab_type": "code",
        "outputId": "7ad39ab6-5eb5-433c-b024-6287f7df7e58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "words"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boy',\n",
              " 'girl',\n",
              " 'king',\n",
              " 'man',\n",
              " 'pretty',\n",
              " 'prince',\n",
              " 'princess',\n",
              " 'queen',\n",
              " 'strong',\n",
              " 'wise',\n",
              " 'woman',\n",
              " 'young'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5dvlC3isdHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c00275b7-36d2-4d85-932d-1a5094b9f8ae"
      },
      "source": [
        "len(words)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct45NXjRANHH",
        "colab_type": "text"
      },
      "source": [
        "# Data Generation\n",
        "we will generate label for each word using skip gram.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgK_sbXTsmVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2int = {}\n",
        "\n",
        "for i,word in enumerate(words):\n",
        "    word2int[word] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frbrd3tdsnGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "807705cf-1b65-4074-9645-7d73f22b936c"
      },
      "source": [
        "word2int"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boy': 7,\n",
              " 'girl': 10,\n",
              " 'king': 1,\n",
              " 'man': 2,\n",
              " 'pretty': 0,\n",
              " 'prince': 8,\n",
              " 'princess': 4,\n",
              " 'queen': 6,\n",
              " 'strong': 3,\n",
              " 'wise': 5,\n",
              " 'woman': 9,\n",
              " 'young': 11}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU_ZdOaKs1T-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = []\n",
        "for sentence in corpus:\n",
        "    sentences.append(sentence.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGWn2uUjs1h1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "afe27994-4e1d-4d50-8608-23251bb4d4f8"
      },
      "source": [
        "sentences"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['king', 'strong', 'man'],\n",
              " ['queen', 'wise', 'woman'],\n",
              " ['boy', 'young', 'man'],\n",
              " ['girl', 'young', 'woman'],\n",
              " ['prince', 'young', 'king'],\n",
              " ['princess', 'young', 'queen'],\n",
              " ['man', 'strong'],\n",
              " ['woman', 'pretty'],\n",
              " ['prince', 'boy', 'king'],\n",
              " ['princess', 'girl', 'queen']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mShcJDE1ANHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WINDOW_SIZE = 2\n",
        "\n",
        "data = []\n",
        "for sentence in sentences:\n",
        "    for idx, word in enumerate(sentence):\n",
        "        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1] : \n",
        "            if neighbor != word:\n",
        "                data.append([word, neighbor])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeVd2JpCtE0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "24bbe6a9-fdec-42b8-9467-58ac3eb4f99a"
      },
      "source": [
        "data[:5]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['king', 'strong'],\n",
              " ['king', 'man'],\n",
              " ['strong', 'king'],\n",
              " ['strong', 'man'],\n",
              " ['man', 'king']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbzm5tyntkAz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "80b6b96f-d420-4d67-a89d-919864cb2073"
      },
      "source": [
        "for text in corpus:\n",
        "    print(text)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "king strong man\n",
            "queen wise woman\n",
            "boy young man\n",
            "girl young woman\n",
            "prince young king\n",
            "princess young queen\n",
            "man strong\n",
            "woman pretty\n",
            "prince boy king\n",
            "princess girl queen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRCLLyNZANHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data, columns = ['input', 'label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSf3CM5ZANHS",
        "colab_type": "code",
        "outputId": "328161c7-4ef6-4632-8a03-8686077e9739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>king</td>\n",
              "      <td>strong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>king</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>strong</td>\n",
              "      <td>king</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>strong</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>man</td>\n",
              "      <td>king</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>man</td>\n",
              "      <td>strong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>queen</td>\n",
              "      <td>wise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>queen</td>\n",
              "      <td>woman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wise</td>\n",
              "      <td>queen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wise</td>\n",
              "      <td>woman</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    input   label\n",
              "0    king  strong\n",
              "1    king     man\n",
              "2  strong    king\n",
              "3  strong     man\n",
              "4     man    king\n",
              "5     man  strong\n",
              "6   queen    wise\n",
              "7   queen   woman\n",
              "8    wise   queen\n",
              "9    wise   woman"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB8PU7etANHX",
        "colab_type": "code",
        "outputId": "192b171a-e729-4940-db17-f11d44ca8b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8LiEQKEANHg",
        "colab_type": "text"
      },
      "source": [
        "# Define Tensorflow Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98XKRJnXuPhZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "outputId": "44717df9-5d40-4c1b-aada-861b51c846cd"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNCkq-ffuRVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a674e21c-5052-4c4e-bd17-9cb3d3929197"
      },
      "source": [
        "ONE_HOT_DIM = len(words); ONE_HOT_DIM"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xbUQ5FbuSUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to convert numbers to one hot vectors\n",
        "def to_one_hot_encoding(data_point_index):\n",
        "    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
        "    one_hot_encoding[data_point_index] = 1\n",
        "    return one_hot_encoding\n",
        "\n",
        "X = [] # input word\n",
        "Y = [] # target word\n",
        "\n",
        "for x, y in zip(df['input'], df['label']):\n",
        "    X.append(to_one_hot_encoding(word2int[ x ]))\n",
        "    Y.append(to_one_hot_encoding(word2int[ y ]))\n",
        "\n",
        "# convert them to numpy arrays\n",
        "X_train = np.asarray(X)\n",
        "Y_train = np.asarray(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27_kXHivuSlq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0a9835b-c5d2-47e3-c4b2-e8a674e8a1f0"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulP0Lg0Ju99E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "363f2ba4-fd39-4249-98a7-5a7d8789029f"
      },
      "source": [
        "Y_train.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GnroykMu-MV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4b0d24f-a9e8-4eb6-9ba6-830ac2e2b4ca"
      },
      "source": [
        "pd.get_dummies(df['input'], drop_first=True).shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LglyEqRIwU8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making placeholders for X_train and Y_train\n",
        "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
        "y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6VrkciSwWZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word embedding will be 2 dimension for 2d visualization\n",
        "EMBEDDING_DIM = 2 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CedCx4eJxemM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03f5eba8-1bfc-41a2-ca5d-f39cf0b409f8"
      },
      "source": [
        "ONE_HOT_DIM"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBXFHKy7xUKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hidden layer: which represents word vector eventually\n",
        "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n",
        "b1 = tf.Variable(tf.random_normal([1])) #bias\n",
        "hidden_layer = tf.add(tf.matmul(x,W1), b1) # X . W1 + b1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VS-a3ssxp-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# output layer\n",
        "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n",
        "b2 = tf.Variable(tf.random_normal([1]))\n",
        "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2)) # soft(h*W2+b2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ihiz7Uex17M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function: cross entropy\n",
        "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd0tOvLZyNwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer\n",
        "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz1bfXvTANHk",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnorLgVWANHm",
        "colab_type": "code",
        "outputId": "1161caad-2afd-456c-8087-84a46caa490a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init) \n",
        "\n",
        "iteration = 20000\n",
        "for i in range(iteration):\n",
        "    # input is X_train which is one hot encoded word\n",
        "    # label is Y_train which is one hot encoded neighbor word\n",
        "    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n",
        "    if i % 3000 == 0:\n",
        "        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 loss is :  3.7877326\n",
            "iteration 3000 loss is :  1.8230131\n",
            "iteration 6000 loss is :  1.7283299\n",
            "iteration 9000 loss is :  1.7001395\n",
            "iteration 12000 loss is :  1.682\n",
            "iteration 15000 loss is :  1.6700953\n",
            "iteration 18000 loss is :  1.6615599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xUpELnfANHs",
        "colab_type": "code",
        "outputId": "f6139d50-c623-4877-e011-c667594cdecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "# Now the hidden layer (W1 + b1) is actually the word look up table\n",
        "vectors = sess.run(W1 + b1)\n",
        "print(vectors)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3.6112041   2.345361  ]\n",
            " [-0.13356537 -0.87538254]\n",
            " [-0.8976615  -4.4182453 ]\n",
            " [ 4.04639    -0.80924565]\n",
            " [-4.822583    3.806186  ]\n",
            " [ 0.5967971   3.5529068 ]\n",
            " [-0.88639855  0.7118491 ]\n",
            " [-0.07870828 -0.9203463 ]\n",
            " [-0.84841835 -5.177806  ]\n",
            " [-0.8586485   0.5323071 ]\n",
            " [-2.0200486   1.6342849 ]\n",
            " [ 0.7108572   0.21704124]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmW0RtnGANHz",
        "colab_type": "text"
      },
      "source": [
        "# word vector in table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLU2Q5WMANH0",
        "colab_type": "code",
        "outputId": "f07b2227-b716-4819-a3f0-96650128c304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
        "w2v_df['word'] = words\n",
        "w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
        "w2v_df"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pretty</td>\n",
              "      <td>3.611204</td>\n",
              "      <td>2.345361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>king</td>\n",
              "      <td>-0.133565</td>\n",
              "      <td>-0.875383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>man</td>\n",
              "      <td>-0.897662</td>\n",
              "      <td>-4.418245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>strong</td>\n",
              "      <td>4.046390</td>\n",
              "      <td>-0.809246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>princess</td>\n",
              "      <td>-4.822583</td>\n",
              "      <td>3.806186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wise</td>\n",
              "      <td>0.596797</td>\n",
              "      <td>3.552907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>queen</td>\n",
              "      <td>-0.886399</td>\n",
              "      <td>0.711849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>boy</td>\n",
              "      <td>-0.078708</td>\n",
              "      <td>-0.920346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>prince</td>\n",
              "      <td>-0.848418</td>\n",
              "      <td>-5.177806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>woman</td>\n",
              "      <td>-0.858648</td>\n",
              "      <td>0.532307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>girl</td>\n",
              "      <td>-2.020049</td>\n",
              "      <td>1.634285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>young</td>\n",
              "      <td>0.710857</td>\n",
              "      <td>0.217041</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        word        x1        x2\n",
              "0     pretty  3.611204  2.345361\n",
              "1       king -0.133565 -0.875383\n",
              "2        man -0.897662 -4.418245\n",
              "3     strong  4.046390 -0.809246\n",
              "4   princess -4.822583  3.806186\n",
              "5       wise  0.596797  3.552907\n",
              "6      queen -0.886399  0.711849\n",
              "7        boy -0.078708 -0.920346\n",
              "8     prince -0.848418 -5.177806\n",
              "9      woman -0.858648  0.532307\n",
              "10      girl -2.020049  1.634285\n",
              "11     young  0.710857  0.217041"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jkr_CapANH5",
        "colab_type": "text"
      },
      "source": [
        "# word vector in 2d chart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxXYrWrtANH6",
        "colab_type": "code",
        "outputId": "4644ec30-9e8a-49cb-ed6b-276ddb5ceda8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
        "    ax.annotate(word, (x1,x2 ))\n",
        "    \n",
        "PADDING = 1.0\n",
        "x_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n",
        "y_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n",
        "x_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n",
        "y_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n",
        " \n",
        "plt.xlim(x_axis_min,x_axis_max)\n",
        "plt.ylim(y_axis_min,y_axis_max)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Urksx-zBDq7",
        "colab_type": "text"
      },
      "source": [
        "# Implementation on Keras - Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zadi93B4ANH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import numpy here\n",
        "import numpy as np\n",
        "\n",
        "# Import a Keras sequential model here\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Import the following Keras Layers: Embedding, Flatten, Dense\n",
        "from keras.layers import Embedding, Flatten, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QQwEJzD4c4E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "aa947b90-bef7-491f-9680-ceaae5ca2382"
      },
      "source": [
        "word2int"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boy': 7,\n",
              " 'girl': 10,\n",
              " 'king': 1,\n",
              " 'man': 2,\n",
              " 'pretty': 0,\n",
              " 'prince': 8,\n",
              " 'princess': 4,\n",
              " 'queen': 6,\n",
              " 'strong': 3,\n",
              " 'wise': 5,\n",
              " 'woman': 9,\n",
              " 'young': 11}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsoOumHn4z3b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3257c8a2-d41c-4f10-f2fa-2af57c86cd4f"
      },
      "source": [
        "word2int_fun = lambda x: word2int[x]\n",
        "word2int_fun('boy')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbQ6sMxT4gd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df['input'].apply(lambda x: word2int[x]).values\n",
        "y = df['label'].apply(lambda x: word2int[x]).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2WBCBdfB2t5",
        "colab_type": "code",
        "outputId": "f29a6d37-ddad-4dd7-9f3f-0f9b75496a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#y[:, None]\n",
        "y.reshape(-1, 1).shape"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKXoacMNDsmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape X and y\n",
        "X = X.reshape(52, 1)\n",
        "y = y.reshape(52, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH73j1yoEXDN",
        "colab_type": "code",
        "outputId": "acedea39-929b-4666-e4ca-4c68b99de89d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1],\n",
              "       [ 1],\n",
              "       [ 3],\n",
              "       [ 3],\n",
              "       [ 2],\n",
              "       [ 2],\n",
              "       [ 6],\n",
              "       [ 6],\n",
              "       [ 5],\n",
              "       [ 5],\n",
              "       [ 9],\n",
              "       [ 9],\n",
              "       [ 7],\n",
              "       [ 7],\n",
              "       [11],\n",
              "       [11],\n",
              "       [ 2],\n",
              "       [ 2],\n",
              "       [10],\n",
              "       [10],\n",
              "       [11],\n",
              "       [11],\n",
              "       [ 9],\n",
              "       [ 9],\n",
              "       [ 8],\n",
              "       [ 8],\n",
              "       [11],\n",
              "       [11],\n",
              "       [ 1],\n",
              "       [ 1],\n",
              "       [ 4],\n",
              "       [ 4],\n",
              "       [11],\n",
              "       [11],\n",
              "       [ 6],\n",
              "       [ 6],\n",
              "       [ 2],\n",
              "       [ 3],\n",
              "       [ 9],\n",
              "       [ 0],\n",
              "       [ 8],\n",
              "       [ 8],\n",
              "       [ 7],\n",
              "       [ 7],\n",
              "       [ 1],\n",
              "       [ 1],\n",
              "       [ 4],\n",
              "       [ 4],\n",
              "       [10],\n",
              "       [10],\n",
              "       [ 6],\n",
              "       [ 6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbqD77c3EXH_",
        "colab_type": "code",
        "outputId": "ceaef041-3ac9-4f5a-c62d-855c57ec814a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M17HumFEs4z",
        "colab_type": "code",
        "outputId": "65019e3f-7930-4af7-f0a7-eddae39dfbd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max(X)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBHy1K7xBxXs",
        "colab_type": "code",
        "outputId": "768bd178-d169-4b5b-8467-7b5fa3c5d314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Define Keras model - Word2Vec \n",
        "model = Sequential([\n",
        "                    Embedding(input_dim=12, output_dim=2, input_length=1),\n",
        "                    Flatten(),\n",
        "                    Dense(12, activation='softmax')                  \n",
        "])\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5zMAR8W9Z50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "efe04dfd-aa4a-47a0-9b6d-4c09bfcad7e1"
      },
      "source": [
        "# Compile model\n",
        "# Loss: https://jovianlin.io/cat-crossentropy-vs-sparse-cat-crossentropy/\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGAQg8TF9pAk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14d5d950-569a-44a3-8537-43055b247513"
      },
      "source": [
        "model.fit(X, y, epochs=1000)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 2.4840\n",
            "Epoch 2/1000\n",
            "52/52 [==============================] - 0s 116us/step - loss: 2.4830\n",
            "Epoch 3/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 2.4821\n",
            "Epoch 4/1000\n",
            "52/52 [==============================] - 0s 152us/step - loss: 2.4813\n",
            "Epoch 5/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 2.4805\n",
            "Epoch 6/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 2.4796\n",
            "Epoch 7/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 2.4788\n",
            "Epoch 8/1000\n",
            "52/52 [==============================] - 0s 118us/step - loss: 2.4780\n",
            "Epoch 9/1000\n",
            "52/52 [==============================] - 0s 118us/step - loss: 2.4771\n",
            "Epoch 10/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 2.4763\n",
            "Epoch 11/1000\n",
            "52/52 [==============================] - 0s 118us/step - loss: 2.4754\n",
            "Epoch 12/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 2.4746\n",
            "Epoch 13/1000\n",
            "52/52 [==============================] - 0s 108us/step - loss: 2.4738\n",
            "Epoch 14/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.4730\n",
            "Epoch 15/1000\n",
            "52/52 [==============================] - 0s 109us/step - loss: 2.4721\n",
            "Epoch 16/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 2.4712\n",
            "Epoch 17/1000\n",
            "52/52 [==============================] - 0s 108us/step - loss: 2.4704\n",
            "Epoch 18/1000\n",
            "52/52 [==============================] - 0s 115us/step - loss: 2.4696\n",
            "Epoch 19/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 2.4687\n",
            "Epoch 20/1000\n",
            "52/52 [==============================] - 0s 118us/step - loss: 2.4679\n",
            "Epoch 21/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 2.4670\n",
            "Epoch 22/1000\n",
            "52/52 [==============================] - 0s 110us/step - loss: 2.4662\n",
            "Epoch 23/1000\n",
            "52/52 [==============================] - 0s 97us/step - loss: 2.4653\n",
            "Epoch 24/1000\n",
            "52/52 [==============================] - 0s 102us/step - loss: 2.4645\n",
            "Epoch 25/1000\n",
            "52/52 [==============================] - 0s 93us/step - loss: 2.4636\n",
            "Epoch 26/1000\n",
            "52/52 [==============================] - 0s 103us/step - loss: 2.4628\n",
            "Epoch 27/1000\n",
            "52/52 [==============================] - 0s 114us/step - loss: 2.4619\n",
            "Epoch 28/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 2.4611\n",
            "Epoch 29/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 2.4602\n",
            "Epoch 30/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 2.4593\n",
            "Epoch 31/1000\n",
            "52/52 [==============================] - 0s 115us/step - loss: 2.4585\n",
            "Epoch 32/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 2.4576\n",
            "Epoch 33/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 2.4567\n",
            "Epoch 34/1000\n",
            "52/52 [==============================] - 0s 110us/step - loss: 2.4559\n",
            "Epoch 35/1000\n",
            "52/52 [==============================] - 0s 114us/step - loss: 2.4550\n",
            "Epoch 36/1000\n",
            "52/52 [==============================] - 0s 115us/step - loss: 2.4541\n",
            "Epoch 37/1000\n",
            "52/52 [==============================] - 0s 112us/step - loss: 2.4531\n",
            "Epoch 38/1000\n",
            "52/52 [==============================] - 0s 119us/step - loss: 2.4522\n",
            "Epoch 39/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 2.4513\n",
            "Epoch 40/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 2.4504\n",
            "Epoch 41/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 2.4495\n",
            "Epoch 42/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 2.4486\n",
            "Epoch 43/1000\n",
            "52/52 [==============================] - 0s 108us/step - loss: 2.4476\n",
            "Epoch 44/1000\n",
            "52/52 [==============================] - 0s 118us/step - loss: 2.4467\n",
            "Epoch 45/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 2.4458\n",
            "Epoch 46/1000\n",
            "52/52 [==============================] - 0s 104us/step - loss: 2.4449\n",
            "Epoch 47/1000\n",
            "52/52 [==============================] - 0s 101us/step - loss: 2.4438\n",
            "Epoch 48/1000\n",
            "52/52 [==============================] - 0s 106us/step - loss: 2.4429\n",
            "Epoch 49/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 2.4419\n",
            "Epoch 50/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 2.4409\n",
            "Epoch 51/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 2.4400\n",
            "Epoch 52/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 2.4390\n",
            "Epoch 53/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 2.4380\n",
            "Epoch 54/1000\n",
            "52/52 [==============================] - 0s 116us/step - loss: 2.4370\n",
            "Epoch 55/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.4360\n",
            "Epoch 56/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 2.4350\n",
            "Epoch 57/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.4340\n",
            "Epoch 58/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 2.4330\n",
            "Epoch 59/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 2.4320\n",
            "Epoch 60/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 2.4309\n",
            "Epoch 61/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 2.4299\n",
            "Epoch 62/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 2.4288\n",
            "Epoch 63/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 2.4277\n",
            "Epoch 64/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 2.4267\n",
            "Epoch 65/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.4256\n",
            "Epoch 66/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 2.4245\n",
            "Epoch 67/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 2.4234\n",
            "Epoch 68/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 2.4224\n",
            "Epoch 69/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.4213\n",
            "Epoch 70/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.4202\n",
            "Epoch 71/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 2.4190\n",
            "Epoch 72/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 2.4179\n",
            "Epoch 73/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.4168\n",
            "Epoch 74/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 2.4157\n",
            "Epoch 75/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 2.4145\n",
            "Epoch 76/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 2.4134\n",
            "Epoch 77/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 2.4122\n",
            "Epoch 78/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 2.4110\n",
            "Epoch 79/1000\n",
            "52/52 [==============================] - 0s 147us/step - loss: 2.4099\n",
            "Epoch 80/1000\n",
            "52/52 [==============================] - 0s 165us/step - loss: 2.4087\n",
            "Epoch 81/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 2.4075\n",
            "Epoch 82/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 2.4063\n",
            "Epoch 83/1000\n",
            "52/52 [==============================] - 0s 129us/step - loss: 2.4051\n",
            "Epoch 84/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 2.4039\n",
            "Epoch 85/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 2.4027\n",
            "Epoch 86/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.4015\n",
            "Epoch 87/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 2.4002\n",
            "Epoch 88/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 2.3990\n",
            "Epoch 89/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 2.3978\n",
            "Epoch 90/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 2.3966\n",
            "Epoch 91/1000\n",
            "52/52 [==============================] - 0s 147us/step - loss: 2.3953\n",
            "Epoch 92/1000\n",
            "52/52 [==============================] - 0s 118us/step - loss: 2.3940\n",
            "Epoch 93/1000\n",
            "52/52 [==============================] - 0s 106us/step - loss: 2.3928\n",
            "Epoch 94/1000\n",
            "52/52 [==============================] - 0s 108us/step - loss: 2.3915\n",
            "Epoch 95/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 2.3902\n",
            "Epoch 96/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 2.3890\n",
            "Epoch 97/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.3876\n",
            "Epoch 98/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 2.3864\n",
            "Epoch 99/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 2.3850\n",
            "Epoch 100/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 2.3838\n",
            "Epoch 101/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 2.3824\n",
            "Epoch 102/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 2.3811\n",
            "Epoch 103/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 2.3798\n",
            "Epoch 104/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.3784\n",
            "Epoch 105/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.3771\n",
            "Epoch 106/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 2.3758\n",
            "Epoch 107/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 2.3744\n",
            "Epoch 108/1000\n",
            "52/52 [==============================] - 0s 129us/step - loss: 2.3730\n",
            "Epoch 109/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 2.3717\n",
            "Epoch 110/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 2.3703\n",
            "Epoch 111/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 2.3689\n",
            "Epoch 112/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 2.3676\n",
            "Epoch 113/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.3662\n",
            "Epoch 114/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 2.3648\n",
            "Epoch 115/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 2.3634\n",
            "Epoch 116/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 2.3621\n",
            "Epoch 117/1000\n",
            "52/52 [==============================] - 0s 113us/step - loss: 2.3607\n",
            "Epoch 118/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 2.3593\n",
            "Epoch 119/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 2.3579\n",
            "Epoch 120/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 2.3565\n",
            "Epoch 121/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 2.3550\n",
            "Epoch 122/1000\n",
            "52/52 [==============================] - 0s 113us/step - loss: 2.3537\n",
            "Epoch 123/1000\n",
            "52/52 [==============================] - 0s 119us/step - loss: 2.3522\n",
            "Epoch 124/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 2.3508\n",
            "Epoch 125/1000\n",
            "52/52 [==============================] - 0s 110us/step - loss: 2.3493\n",
            "Epoch 126/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 2.3479\n",
            "Epoch 127/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 2.3465\n",
            "Epoch 128/1000\n",
            "52/52 [==============================] - 0s 115us/step - loss: 2.3450\n",
            "Epoch 129/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 2.3435\n",
            "Epoch 130/1000\n",
            "52/52 [==============================] - 0s 116us/step - loss: 2.3420\n",
            "Epoch 131/1000\n",
            "52/52 [==============================] - 0s 333us/step - loss: 2.3406\n",
            "Epoch 132/1000\n",
            "52/52 [==============================] - 0s 155us/step - loss: 2.3391\n",
            "Epoch 133/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 2.3377\n",
            "Epoch 134/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 2.3362\n",
            "Epoch 135/1000\n",
            "52/52 [==============================] - 0s 112us/step - loss: 2.3347\n",
            "Epoch 136/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 2.3332\n",
            "Epoch 137/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.3318\n",
            "Epoch 138/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 2.3302\n",
            "Epoch 139/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 2.3288\n",
            "Epoch 140/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 2.3273\n",
            "Epoch 141/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 2.3258\n",
            "Epoch 142/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 2.3242\n",
            "Epoch 143/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 2.3228\n",
            "Epoch 144/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 2.3213\n",
            "Epoch 145/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 2.3198\n",
            "Epoch 146/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.3182\n",
            "Epoch 147/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.3167\n",
            "Epoch 148/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 2.3152\n",
            "Epoch 149/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 2.3138\n",
            "Epoch 150/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 2.3121\n",
            "Epoch 151/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 2.3106\n",
            "Epoch 152/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 2.3092\n",
            "Epoch 153/1000\n",
            "52/52 [==============================] - 0s 102us/step - loss: 2.3077\n",
            "Epoch 154/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 2.3061\n",
            "Epoch 155/1000\n",
            "52/52 [==============================] - 0s 147us/step - loss: 2.3046\n",
            "Epoch 156/1000\n",
            "52/52 [==============================] - 0s 154us/step - loss: 2.3032\n",
            "Epoch 157/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 2.3016\n",
            "Epoch 158/1000\n",
            "52/52 [==============================] - 0s 115us/step - loss: 2.3001\n",
            "Epoch 159/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 2.2986\n",
            "Epoch 160/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 2.2971\n",
            "Epoch 161/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.2955\n",
            "Epoch 162/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 2.2940\n",
            "Epoch 163/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 2.2925\n",
            "Epoch 164/1000\n",
            "52/52 [==============================] - 0s 147us/step - loss: 2.2910\n",
            "Epoch 165/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 2.2895\n",
            "Epoch 166/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 2.2879\n",
            "Epoch 167/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 2.2863\n",
            "Epoch 168/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 2.2849\n",
            "Epoch 169/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 2.2833\n",
            "Epoch 170/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 2.2818\n",
            "Epoch 171/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 2.2803\n",
            "Epoch 172/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 2.2788\n",
            "Epoch 173/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 2.2772\n",
            "Epoch 174/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.2757\n",
            "Epoch 175/1000\n",
            "52/52 [==============================] - 0s 119us/step - loss: 2.2741\n",
            "Epoch 176/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 2.2726\n",
            "Epoch 177/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 2.2711\n",
            "Epoch 178/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 2.2695\n",
            "Epoch 179/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.2681\n",
            "Epoch 180/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 2.2666\n",
            "Epoch 181/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 2.2650\n",
            "Epoch 182/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 2.2635\n",
            "Epoch 183/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 2.2619\n",
            "Epoch 184/1000\n",
            "52/52 [==============================] - 0s 114us/step - loss: 2.2604\n",
            "Epoch 185/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.2590\n",
            "Epoch 186/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 2.2574\n",
            "Epoch 187/1000\n",
            "52/52 [==============================] - 0s 157us/step - loss: 2.2559\n",
            "Epoch 188/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 2.2544\n",
            "Epoch 189/1000\n",
            "52/52 [==============================] - 0s 169us/step - loss: 2.2529\n",
            "Epoch 190/1000\n",
            "52/52 [==============================] - 0s 165us/step - loss: 2.2513\n",
            "Epoch 191/1000\n",
            "52/52 [==============================] - 0s 186us/step - loss: 2.2499\n",
            "Epoch 192/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 2.2483\n",
            "Epoch 193/1000\n",
            "52/52 [==============================] - 0s 159us/step - loss: 2.2468\n",
            "Epoch 194/1000\n",
            "52/52 [==============================] - 0s 176us/step - loss: 2.2453\n",
            "Epoch 195/1000\n",
            "52/52 [==============================] - 0s 176us/step - loss: 2.2439\n",
            "Epoch 196/1000\n",
            "52/52 [==============================] - 0s 174us/step - loss: 2.2424\n",
            "Epoch 197/1000\n",
            "52/52 [==============================] - 0s 202us/step - loss: 2.2408\n",
            "Epoch 198/1000\n",
            "52/52 [==============================] - 0s 182us/step - loss: 2.2394\n",
            "Epoch 199/1000\n",
            "52/52 [==============================] - 0s 103us/step - loss: 2.2380\n",
            "Epoch 200/1000\n",
            "52/52 [==============================] - 0s 170us/step - loss: 2.2364\n",
            "Epoch 201/1000\n",
            "52/52 [==============================] - 0s 178us/step - loss: 2.2349\n",
            "Epoch 202/1000\n",
            "52/52 [==============================] - 0s 177us/step - loss: 2.2335\n",
            "Epoch 203/1000\n",
            "52/52 [==============================] - 0s 179us/step - loss: 2.2320\n",
            "Epoch 204/1000\n",
            "52/52 [==============================] - 0s 201us/step - loss: 2.2306\n",
            "Epoch 205/1000\n",
            "52/52 [==============================] - 0s 193us/step - loss: 2.2290\n",
            "Epoch 206/1000\n",
            "52/52 [==============================] - 0s 194us/step - loss: 2.2276\n",
            "Epoch 207/1000\n",
            "52/52 [==============================] - 0s 178us/step - loss: 2.2261\n",
            "Epoch 208/1000\n",
            "52/52 [==============================] - 0s 182us/step - loss: 2.2247\n",
            "Epoch 209/1000\n",
            "52/52 [==============================] - 0s 180us/step - loss: 2.2232\n",
            "Epoch 210/1000\n",
            "52/52 [==============================] - 0s 201us/step - loss: 2.2218\n",
            "Epoch 211/1000\n",
            "52/52 [==============================] - 0s 193us/step - loss: 2.2202\n",
            "Epoch 212/1000\n",
            "52/52 [==============================] - 0s 168us/step - loss: 2.2188\n",
            "Epoch 213/1000\n",
            "52/52 [==============================] - 0s 181us/step - loss: 2.2174\n",
            "Epoch 214/1000\n",
            "52/52 [==============================] - 0s 182us/step - loss: 2.2159\n",
            "Epoch 215/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 2.2145\n",
            "Epoch 216/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 2.2130\n",
            "Epoch 217/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 2.2115\n",
            "Epoch 218/1000\n",
            "52/52 [==============================] - 0s 159us/step - loss: 2.2102\n",
            "Epoch 219/1000\n",
            "52/52 [==============================] - 0s 107us/step - loss: 2.2087\n",
            "Epoch 220/1000\n",
            "52/52 [==============================] - 0s 154us/step - loss: 2.2072\n",
            "Epoch 221/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 2.2059\n",
            "Epoch 222/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 2.2044\n",
            "Epoch 223/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 2.2030\n",
            "Epoch 224/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 2.2015\n",
            "Epoch 225/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 2.2001\n",
            "Epoch 226/1000\n",
            "52/52 [==============================] - 0s 116us/step - loss: 2.1987\n",
            "Epoch 227/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 2.1973\n",
            "Epoch 228/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 2.1959\n",
            "Epoch 229/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 2.1944\n",
            "Epoch 230/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 2.1931\n",
            "Epoch 231/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 2.1917\n",
            "Epoch 232/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 2.1903\n",
            "Epoch 233/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 2.1889\n",
            "Epoch 234/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 2.1875\n",
            "Epoch 235/1000\n",
            "52/52 [==============================] - 0s 119us/step - loss: 2.1862\n",
            "Epoch 236/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 2.1847\n",
            "Epoch 237/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.1834\n",
            "Epoch 238/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 2.1820\n",
            "Epoch 239/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 2.1807\n",
            "Epoch 240/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 2.1793\n",
            "Epoch 241/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 2.1780\n",
            "Epoch 242/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 2.1766\n",
            "Epoch 243/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 2.1753\n",
            "Epoch 244/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 2.1739\n",
            "Epoch 245/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 2.1726\n",
            "Epoch 246/1000\n",
            "52/52 [==============================] - 0s 154us/step - loss: 2.1713\n",
            "Epoch 247/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 2.1699\n",
            "Epoch 248/1000\n",
            "52/52 [==============================] - 0s 113us/step - loss: 2.1686\n",
            "Epoch 249/1000\n",
            "52/52 [==============================] - 0s 154us/step - loss: 2.1673\n",
            "Epoch 250/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 2.1660\n",
            "Epoch 251/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.1647\n",
            "Epoch 252/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 2.1634\n",
            "Epoch 253/1000\n",
            "52/52 [==============================] - 0s 110us/step - loss: 2.1621\n",
            "Epoch 254/1000\n",
            "52/52 [==============================] - 0s 165us/step - loss: 2.1608\n",
            "Epoch 255/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.1595\n",
            "Epoch 256/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 2.1583\n",
            "Epoch 257/1000\n",
            "52/52 [==============================] - 0s 100us/step - loss: 2.1570\n",
            "Epoch 258/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 2.1557\n",
            "Epoch 259/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 2.1545\n",
            "Epoch 260/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 2.1532\n",
            "Epoch 261/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 2.1520\n",
            "Epoch 262/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 2.1506\n",
            "Epoch 263/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 2.1494\n",
            "Epoch 264/1000\n",
            "52/52 [==============================] - 0s 159us/step - loss: 2.1482\n",
            "Epoch 265/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 2.1470\n",
            "Epoch 266/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 2.1457\n",
            "Epoch 267/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 2.1444\n",
            "Epoch 268/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 2.1432\n",
            "Epoch 269/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 2.1420\n",
            "Epoch 270/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 2.1408\n",
            "Epoch 271/1000\n",
            "52/52 [==============================] - 0s 116us/step - loss: 2.1396\n",
            "Epoch 272/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 2.1383\n",
            "Epoch 273/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 2.1372\n",
            "Epoch 274/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 2.1359\n",
            "Epoch 275/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 2.1347\n",
            "Epoch 276/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 2.1335\n",
            "Epoch 277/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.1323\n",
            "Epoch 278/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 2.1311\n",
            "Epoch 279/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 2.1299\n",
            "Epoch 280/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 2.1287\n",
            "Epoch 281/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 2.1276\n",
            "Epoch 282/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 2.1265\n",
            "Epoch 283/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 2.1252\n",
            "Epoch 284/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 2.1242\n",
            "Epoch 285/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 2.1229\n",
            "Epoch 286/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 2.1218\n",
            "Epoch 287/1000\n",
            "52/52 [==============================] - 0s 157us/step - loss: 2.1207\n",
            "Epoch 288/1000\n",
            "52/52 [==============================] - 0s 152us/step - loss: 2.1196\n",
            "Epoch 289/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 2.1184\n",
            "Epoch 290/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 2.1173\n",
            "Epoch 291/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 2.1162\n",
            "Epoch 292/1000\n",
            "52/52 [==============================] - 0s 216us/step - loss: 2.1151\n",
            "Epoch 293/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 2.1139\n",
            "Epoch 294/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 2.1128\n",
            "Epoch 295/1000\n",
            "52/52 [==============================] - 0s 153us/step - loss: 2.1117\n",
            "Epoch 296/1000\n",
            "52/52 [==============================] - 0s 151us/step - loss: 2.1107\n",
            "Epoch 297/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 2.1095\n",
            "Epoch 298/1000\n",
            "52/52 [==============================] - 0s 151us/step - loss: 2.1085\n",
            "Epoch 299/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 2.1073\n",
            "Epoch 300/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 2.1063\n",
            "Epoch 301/1000\n",
            "52/52 [==============================] - 0s 159us/step - loss: 2.1052\n",
            "Epoch 302/1000\n",
            "52/52 [==============================] - 0s 213us/step - loss: 2.1041\n",
            "Epoch 303/1000\n",
            "52/52 [==============================] - 0s 191us/step - loss: 2.1030\n",
            "Epoch 304/1000\n",
            "52/52 [==============================] - 0s 207us/step - loss: 2.1021\n",
            "Epoch 305/1000\n",
            "52/52 [==============================] - 0s 188us/step - loss: 2.1009\n",
            "Epoch 306/1000\n",
            "52/52 [==============================] - 0s 215us/step - loss: 2.0999\n",
            "Epoch 307/1000\n",
            "52/52 [==============================] - 0s 211us/step - loss: 2.0988\n",
            "Epoch 308/1000\n",
            "52/52 [==============================] - 0s 215us/step - loss: 2.0978\n",
            "Epoch 309/1000\n",
            "52/52 [==============================] - 0s 201us/step - loss: 2.0967\n",
            "Epoch 310/1000\n",
            "52/52 [==============================] - 0s 182us/step - loss: 2.0957\n",
            "Epoch 311/1000\n",
            "52/52 [==============================] - 0s 184us/step - loss: 2.0947\n",
            "Epoch 312/1000\n",
            "52/52 [==============================] - 0s 235us/step - loss: 2.0937\n",
            "Epoch 313/1000\n",
            "52/52 [==============================] - 0s 181us/step - loss: 2.0926\n",
            "Epoch 314/1000\n",
            "52/52 [==============================] - 0s 210us/step - loss: 2.0916\n",
            "Epoch 315/1000\n",
            "52/52 [==============================] - 0s 219us/step - loss: 2.0906\n",
            "Epoch 316/1000\n",
            "52/52 [==============================] - 0s 216us/step - loss: 2.0897\n",
            "Epoch 317/1000\n",
            "52/52 [==============================] - 0s 215us/step - loss: 2.0886\n",
            "Epoch 318/1000\n",
            "52/52 [==============================] - 0s 267us/step - loss: 2.0876\n",
            "Epoch 319/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 2.0865\n",
            "Epoch 320/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 2.0856\n",
            "Epoch 321/1000\n",
            "52/52 [==============================] - 0s 147us/step - loss: 2.0846\n",
            "Epoch 322/1000\n",
            "52/52 [==============================] - 0s 165us/step - loss: 2.0836\n",
            "Epoch 323/1000\n",
            "52/52 [==============================] - 0s 119us/step - loss: 2.0826\n",
            "Epoch 324/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 2.0816\n",
            "Epoch 325/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 2.0806\n",
            "Epoch 326/1000\n",
            "52/52 [==============================] - 0s 223us/step - loss: 2.0796\n",
            "Epoch 327/1000\n",
            "52/52 [==============================] - 0s 199us/step - loss: 2.0786\n",
            "Epoch 328/1000\n",
            "52/52 [==============================] - 0s 202us/step - loss: 2.0777\n",
            "Epoch 329/1000\n",
            "52/52 [==============================] - 0s 171us/step - loss: 2.0767\n",
            "Epoch 330/1000\n",
            "52/52 [==============================] - 0s 188us/step - loss: 2.0758\n",
            "Epoch 331/1000\n",
            "52/52 [==============================] - 0s 174us/step - loss: 2.0747\n",
            "Epoch 332/1000\n",
            "52/52 [==============================] - 0s 180us/step - loss: 2.0737\n",
            "Epoch 333/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 2.0728\n",
            "Epoch 334/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 2.0718\n",
            "Epoch 335/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 2.0709\n",
            "Epoch 336/1000\n",
            "52/52 [==============================] - 0s 360us/step - loss: 2.0699\n",
            "Epoch 337/1000\n",
            "52/52 [==============================] - 0s 223us/step - loss: 2.0690\n",
            "Epoch 338/1000\n",
            "52/52 [==============================] - 0s 196us/step - loss: 2.0680\n",
            "Epoch 339/1000\n",
            "52/52 [==============================] - 0s 204us/step - loss: 2.0670\n",
            "Epoch 340/1000\n",
            "52/52 [==============================] - 0s 229us/step - loss: 2.0661\n",
            "Epoch 341/1000\n",
            "52/52 [==============================] - 0s 294us/step - loss: 2.0652\n",
            "Epoch 342/1000\n",
            "52/52 [==============================] - 0s 202us/step - loss: 2.0642\n",
            "Epoch 343/1000\n",
            "52/52 [==============================] - 0s 213us/step - loss: 2.0633\n",
            "Epoch 344/1000\n",
            "52/52 [==============================] - 0s 183us/step - loss: 2.0623\n",
            "Epoch 345/1000\n",
            "52/52 [==============================] - 0s 224us/step - loss: 2.0614\n",
            "Epoch 346/1000\n",
            "52/52 [==============================] - 0s 200us/step - loss: 2.0605\n",
            "Epoch 347/1000\n",
            "52/52 [==============================] - 0s 204us/step - loss: 2.0595\n",
            "Epoch 348/1000\n",
            "52/52 [==============================] - 0s 207us/step - loss: 2.0586\n",
            "Epoch 349/1000\n",
            "52/52 [==============================] - 0s 233us/step - loss: 2.0577\n",
            "Epoch 350/1000\n",
            "52/52 [==============================] - 0s 170us/step - loss: 2.0568\n",
            "Epoch 351/1000\n",
            "52/52 [==============================] - 0s 314us/step - loss: 2.0558\n",
            "Epoch 352/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 2.0549\n",
            "Epoch 353/1000\n",
            "52/52 [==============================] - 0s 192us/step - loss: 2.0540\n",
            "Epoch 354/1000\n",
            "52/52 [==============================] - 0s 174us/step - loss: 2.0531\n",
            "Epoch 355/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 2.0522\n",
            "Epoch 356/1000\n",
            "52/52 [==============================] - 0s 167us/step - loss: 2.0513\n",
            "Epoch 357/1000\n",
            "52/52 [==============================] - 0s 182us/step - loss: 2.0503\n",
            "Epoch 358/1000\n",
            "52/52 [==============================] - 0s 182us/step - loss: 2.0495\n",
            "Epoch 359/1000\n",
            "52/52 [==============================] - 0s 162us/step - loss: 2.0486\n",
            "Epoch 360/1000\n",
            "52/52 [==============================] - 0s 163us/step - loss: 2.0477\n",
            "Epoch 361/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 2.0468\n",
            "Epoch 362/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 2.0460\n",
            "Epoch 363/1000\n",
            "52/52 [==============================] - 0s 167us/step - loss: 2.0451\n",
            "Epoch 364/1000\n",
            "52/52 [==============================] - 0s 184us/step - loss: 2.0443\n",
            "Epoch 365/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 2.0433\n",
            "Epoch 366/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 2.0424\n",
            "Epoch 367/1000\n",
            "52/52 [==============================] - 0s 167us/step - loss: 2.0416\n",
            "Epoch 368/1000\n",
            "52/52 [==============================] - 0s 177us/step - loss: 2.0408\n",
            "Epoch 369/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 2.0399\n",
            "Epoch 370/1000\n",
            "52/52 [==============================] - 0s 178us/step - loss: 2.0390\n",
            "Epoch 371/1000\n",
            "52/52 [==============================] - 0s 161us/step - loss: 2.0381\n",
            "Epoch 372/1000\n",
            "52/52 [==============================] - 0s 151us/step - loss: 2.0373\n",
            "Epoch 373/1000\n",
            "52/52 [==============================] - 0s 179us/step - loss: 2.0365\n",
            "Epoch 374/1000\n",
            "52/52 [==============================] - 0s 163us/step - loss: 2.0356\n",
            "Epoch 375/1000\n",
            "52/52 [==============================] - 0s 176us/step - loss: 2.0348\n",
            "Epoch 376/1000\n",
            "52/52 [==============================] - 0s 171us/step - loss: 2.0339\n",
            "Epoch 377/1000\n",
            "52/52 [==============================] - 0s 152us/step - loss: 2.0331\n",
            "Epoch 378/1000\n",
            "52/52 [==============================] - 0s 198us/step - loss: 2.0322\n",
            "Epoch 379/1000\n",
            "52/52 [==============================] - 0s 172us/step - loss: 2.0314\n",
            "Epoch 380/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 2.0307\n",
            "Epoch 381/1000\n",
            "52/52 [==============================] - 0s 188us/step - loss: 2.0298\n",
            "Epoch 382/1000\n",
            "52/52 [==============================] - 0s 174us/step - loss: 2.0290\n",
            "Epoch 383/1000\n",
            "52/52 [==============================] - 0s 164us/step - loss: 2.0281\n",
            "Epoch 384/1000\n",
            "52/52 [==============================] - 0s 204us/step - loss: 2.0273\n",
            "Epoch 385/1000\n",
            "52/52 [==============================] - 0s 184us/step - loss: 2.0265\n",
            "Epoch 386/1000\n",
            "52/52 [==============================] - 0s 200us/step - loss: 2.0257\n",
            "Epoch 387/1000\n",
            "52/52 [==============================] - 0s 186us/step - loss: 2.0249\n",
            "Epoch 388/1000\n",
            "52/52 [==============================] - 0s 484us/step - loss: 2.0240\n",
            "Epoch 389/1000\n",
            "52/52 [==============================] - 0s 387us/step - loss: 2.0232\n",
            "Epoch 390/1000\n",
            "52/52 [==============================] - 0s 236us/step - loss: 2.0224\n",
            "Epoch 391/1000\n",
            "52/52 [==============================] - 0s 190us/step - loss: 2.0216\n",
            "Epoch 392/1000\n",
            "52/52 [==============================] - 0s 214us/step - loss: 2.0208\n",
            "Epoch 393/1000\n",
            "52/52 [==============================] - 0s 228us/step - loss: 2.0200\n",
            "Epoch 394/1000\n",
            "52/52 [==============================] - 0s 212us/step - loss: 2.0192\n",
            "Epoch 395/1000\n",
            "52/52 [==============================] - 0s 179us/step - loss: 2.0184\n",
            "Epoch 396/1000\n",
            "52/52 [==============================] - 0s 171us/step - loss: 2.0176\n",
            "Epoch 397/1000\n",
            "52/52 [==============================] - 0s 155us/step - loss: 2.0168\n",
            "Epoch 398/1000\n",
            "52/52 [==============================] - 0s 205us/step - loss: 2.0160\n",
            "Epoch 399/1000\n",
            "52/52 [==============================] - 0s 177us/step - loss: 2.0153\n",
            "Epoch 400/1000\n",
            "52/52 [==============================] - 0s 179us/step - loss: 2.0145\n",
            "Epoch 401/1000\n",
            "52/52 [==============================] - 0s 189us/step - loss: 2.0137\n",
            "Epoch 402/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 2.0129\n",
            "Epoch 403/1000\n",
            "52/52 [==============================] - 0s 181us/step - loss: 2.0122\n",
            "Epoch 404/1000\n",
            "52/52 [==============================] - 0s 171us/step - loss: 2.0114\n",
            "Epoch 405/1000\n",
            "52/52 [==============================] - 0s 162us/step - loss: 2.0106\n",
            "Epoch 406/1000\n",
            "52/52 [==============================] - 0s 195us/step - loss: 2.0099\n",
            "Epoch 407/1000\n",
            "52/52 [==============================] - 0s 176us/step - loss: 2.0091\n",
            "Epoch 408/1000\n",
            "52/52 [==============================] - 0s 197us/step - loss: 2.0083\n",
            "Epoch 409/1000\n",
            "52/52 [==============================] - 0s 220us/step - loss: 2.0076\n",
            "Epoch 410/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 2.0069\n",
            "Epoch 411/1000\n",
            "52/52 [==============================] - 0s 203us/step - loss: 2.0060\n",
            "Epoch 412/1000\n",
            "52/52 [==============================] - 0s 177us/step - loss: 2.0053\n",
            "Epoch 413/1000\n",
            "52/52 [==============================] - 0s 188us/step - loss: 2.0045\n",
            "Epoch 414/1000\n",
            "52/52 [==============================] - 0s 217us/step - loss: 2.0038\n",
            "Epoch 415/1000\n",
            "52/52 [==============================] - 0s 191us/step - loss: 2.0030\n",
            "Epoch 416/1000\n",
            "52/52 [==============================] - 0s 178us/step - loss: 2.0023\n",
            "Epoch 417/1000\n",
            "52/52 [==============================] - 0s 201us/step - loss: 2.0015\n",
            "Epoch 418/1000\n",
            "52/52 [==============================] - 0s 180us/step - loss: 2.0008\n",
            "Epoch 419/1000\n",
            "52/52 [==============================] - 0s 209us/step - loss: 2.0000\n",
            "Epoch 420/1000\n",
            "52/52 [==============================] - 0s 188us/step - loss: 1.9993\n",
            "Epoch 421/1000\n",
            "52/52 [==============================] - 0s 187us/step - loss: 1.9985\n",
            "Epoch 422/1000\n",
            "52/52 [==============================] - 0s 202us/step - loss: 1.9978\n",
            "Epoch 423/1000\n",
            "52/52 [==============================] - 0s 184us/step - loss: 1.9971\n",
            "Epoch 424/1000\n",
            "52/52 [==============================] - 0s 159us/step - loss: 1.9963\n",
            "Epoch 425/1000\n",
            "52/52 [==============================] - 0s 189us/step - loss: 1.9956\n",
            "Epoch 426/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 1.9949\n",
            "Epoch 427/1000\n",
            "52/52 [==============================] - 0s 152us/step - loss: 1.9941\n",
            "Epoch 428/1000\n",
            "52/52 [==============================] - 0s 189us/step - loss: 1.9934\n",
            "Epoch 429/1000\n",
            "52/52 [==============================] - 0s 179us/step - loss: 1.9926\n",
            "Epoch 430/1000\n",
            "52/52 [==============================] - 0s 190us/step - loss: 1.9919\n",
            "Epoch 431/1000\n",
            "52/52 [==============================] - 0s 182us/step - loss: 1.9912\n",
            "Epoch 432/1000\n",
            "52/52 [==============================] - 0s 244us/step - loss: 1.9904\n",
            "Epoch 433/1000\n",
            "52/52 [==============================] - 0s 190us/step - loss: 1.9897\n",
            "Epoch 434/1000\n",
            "52/52 [==============================] - 0s 211us/step - loss: 1.9890\n",
            "Epoch 435/1000\n",
            "52/52 [==============================] - 0s 208us/step - loss: 1.9883\n",
            "Epoch 436/1000\n",
            "52/52 [==============================] - 0s 193us/step - loss: 1.9876\n",
            "Epoch 437/1000\n",
            "52/52 [==============================] - 0s 212us/step - loss: 1.9867\n",
            "Epoch 438/1000\n",
            "52/52 [==============================] - 0s 215us/step - loss: 1.9860\n",
            "Epoch 439/1000\n",
            "52/52 [==============================] - 0s 192us/step - loss: 1.9853\n",
            "Epoch 440/1000\n",
            "52/52 [==============================] - 0s 180us/step - loss: 1.9847\n",
            "Epoch 441/1000\n",
            "52/52 [==============================] - 0s 264us/step - loss: 1.9839\n",
            "Epoch 442/1000\n",
            "52/52 [==============================] - 0s 157us/step - loss: 1.9832\n",
            "Epoch 443/1000\n",
            "52/52 [==============================] - 0s 167us/step - loss: 1.9824\n",
            "Epoch 444/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 1.9817\n",
            "Epoch 445/1000\n",
            "52/52 [==============================] - 0s 114us/step - loss: 1.9810\n",
            "Epoch 446/1000\n",
            "52/52 [==============================] - 0s 216us/step - loss: 1.9804\n",
            "Epoch 447/1000\n",
            "52/52 [==============================] - 0s 248us/step - loss: 1.9797\n",
            "Epoch 448/1000\n",
            "52/52 [==============================] - 0s 179us/step - loss: 1.9790\n",
            "Epoch 449/1000\n",
            "52/52 [==============================] - 0s 205us/step - loss: 1.9782\n",
            "Epoch 450/1000\n",
            "52/52 [==============================] - 0s 213us/step - loss: 1.9776\n",
            "Epoch 451/1000\n",
            "52/52 [==============================] - 0s 175us/step - loss: 1.9768\n",
            "Epoch 452/1000\n",
            "52/52 [==============================] - 0s 190us/step - loss: 1.9762\n",
            "Epoch 453/1000\n",
            "52/52 [==============================] - 0s 167us/step - loss: 1.9755\n",
            "Epoch 454/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.9748\n",
            "Epoch 455/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.9741\n",
            "Epoch 456/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.9733\n",
            "Epoch 457/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.9727\n",
            "Epoch 458/1000\n",
            "52/52 [==============================] - 0s 92us/step - loss: 1.9720\n",
            "Epoch 459/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 1.9713\n",
            "Epoch 460/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 1.9706\n",
            "Epoch 461/1000\n",
            "52/52 [==============================] - 0s 155us/step - loss: 1.9699\n",
            "Epoch 462/1000\n",
            "52/52 [==============================] - 0s 164us/step - loss: 1.9692\n",
            "Epoch 463/1000\n",
            "52/52 [==============================] - 0s 152us/step - loss: 1.9686\n",
            "Epoch 464/1000\n",
            "52/52 [==============================] - 0s 152us/step - loss: 1.9680\n",
            "Epoch 465/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 1.9673\n",
            "Epoch 466/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 1.9665\n",
            "Epoch 467/1000\n",
            "52/52 [==============================] - 0s 163us/step - loss: 1.9659\n",
            "Epoch 468/1000\n",
            "52/52 [==============================] - 0s 207us/step - loss: 1.9653\n",
            "Epoch 469/1000\n",
            "52/52 [==============================] - 0s 165us/step - loss: 1.9645\n",
            "Epoch 470/1000\n",
            "52/52 [==============================] - 0s 172us/step - loss: 1.9639\n",
            "Epoch 471/1000\n",
            "52/52 [==============================] - 0s 194us/step - loss: 1.9632\n",
            "Epoch 472/1000\n",
            "52/52 [==============================] - 0s 231us/step - loss: 1.9625\n",
            "Epoch 473/1000\n",
            "52/52 [==============================] - 0s 185us/step - loss: 1.9619\n",
            "Epoch 474/1000\n",
            "52/52 [==============================] - 0s 252us/step - loss: 1.9611\n",
            "Epoch 475/1000\n",
            "52/52 [==============================] - 0s 196us/step - loss: 1.9605\n",
            "Epoch 476/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.9598\n",
            "Epoch 477/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 1.9591\n",
            "Epoch 478/1000\n",
            "52/52 [==============================] - 0s 104us/step - loss: 1.9585\n",
            "Epoch 479/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 1.9579\n",
            "Epoch 480/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 1.9572\n",
            "Epoch 481/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 1.9565\n",
            "Epoch 482/1000\n",
            "52/52 [==============================] - 0s 105us/step - loss: 1.9559\n",
            "Epoch 483/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 1.9553\n",
            "Epoch 484/1000\n",
            "52/52 [==============================] - 0s 124us/step - loss: 1.9546\n",
            "Epoch 485/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.9539\n",
            "Epoch 486/1000\n",
            "52/52 [==============================] - 0s 116us/step - loss: 1.9534\n",
            "Epoch 487/1000\n",
            "52/52 [==============================] - 0s 173us/step - loss: 1.9527\n",
            "Epoch 488/1000\n",
            "52/52 [==============================] - 0s 116us/step - loss: 1.9520\n",
            "Epoch 489/1000\n",
            "52/52 [==============================] - 0s 97us/step - loss: 1.9514\n",
            "Epoch 490/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.9508\n",
            "Epoch 491/1000\n",
            "52/52 [==============================] - 0s 105us/step - loss: 1.9501\n",
            "Epoch 492/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.9495\n",
            "Epoch 493/1000\n",
            "52/52 [==============================] - 0s 94us/step - loss: 1.9489\n",
            "Epoch 494/1000\n",
            "52/52 [==============================] - 0s 112us/step - loss: 1.9482\n",
            "Epoch 495/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 1.9476\n",
            "Epoch 496/1000\n",
            "52/52 [==============================] - 0s 105us/step - loss: 1.9470\n",
            "Epoch 497/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.9464\n",
            "Epoch 498/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 1.9458\n",
            "Epoch 499/1000\n",
            "52/52 [==============================] - 0s 147us/step - loss: 1.9452\n",
            "Epoch 500/1000\n",
            "52/52 [==============================] - 0s 151us/step - loss: 1.9445\n",
            "Epoch 501/1000\n",
            "52/52 [==============================] - 0s 179us/step - loss: 1.9439\n",
            "Epoch 502/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 1.9433\n",
            "Epoch 503/1000\n",
            "52/52 [==============================] - 0s 161us/step - loss: 1.9427\n",
            "Epoch 504/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.9421\n",
            "Epoch 505/1000\n",
            "52/52 [==============================] - 0s 164us/step - loss: 1.9415\n",
            "Epoch 506/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.9408\n",
            "Epoch 507/1000\n",
            "52/52 [==============================] - 0s 129us/step - loss: 1.9402\n",
            "Epoch 508/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 1.9396\n",
            "Epoch 509/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 1.9390\n",
            "Epoch 510/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.9384\n",
            "Epoch 511/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.9379\n",
            "Epoch 512/1000\n",
            "52/52 [==============================] - 0s 152us/step - loss: 1.9371\n",
            "Epoch 513/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.9366\n",
            "Epoch 514/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 1.9360\n",
            "Epoch 515/1000\n",
            "52/52 [==============================] - 0s 163us/step - loss: 1.9354\n",
            "Epoch 516/1000\n",
            "52/52 [==============================] - 0s 227us/step - loss: 1.9348\n",
            "Epoch 517/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 1.9342\n",
            "Epoch 518/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.9335\n",
            "Epoch 519/1000\n",
            "52/52 [==============================] - 0s 109us/step - loss: 1.9331\n",
            "Epoch 520/1000\n",
            "52/52 [==============================] - 0s 152us/step - loss: 1.9324\n",
            "Epoch 521/1000\n",
            "52/52 [==============================] - 0s 173us/step - loss: 1.9319\n",
            "Epoch 522/1000\n",
            "52/52 [==============================] - 0s 153us/step - loss: 1.9312\n",
            "Epoch 523/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 1.9307\n",
            "Epoch 524/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 1.9301\n",
            "Epoch 525/1000\n",
            "52/52 [==============================] - 0s 151us/step - loss: 1.9295\n",
            "Epoch 526/1000\n",
            "52/52 [==============================] - 0s 129us/step - loss: 1.9289\n",
            "Epoch 527/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.9284\n",
            "Epoch 528/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.9278\n",
            "Epoch 529/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 1.9271\n",
            "Epoch 530/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 1.9267\n",
            "Epoch 531/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 1.9260\n",
            "Epoch 532/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 1.9254\n",
            "Epoch 533/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 1.9249\n",
            "Epoch 534/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.9243\n",
            "Epoch 535/1000\n",
            "52/52 [==============================] - 0s 114us/step - loss: 1.9237\n",
            "Epoch 536/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 1.9232\n",
            "Epoch 537/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 1.9226\n",
            "Epoch 538/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.9220\n",
            "Epoch 539/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.9214\n",
            "Epoch 540/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 1.9209\n",
            "Epoch 541/1000\n",
            "52/52 [==============================] - 0s 154us/step - loss: 1.9203\n",
            "Epoch 542/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.9197\n",
            "Epoch 543/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 1.9192\n",
            "Epoch 544/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 1.9186\n",
            "Epoch 545/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.9181\n",
            "Epoch 546/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.9174\n",
            "Epoch 547/1000\n",
            "52/52 [==============================] - 0s 110us/step - loss: 1.9169\n",
            "Epoch 548/1000\n",
            "52/52 [==============================] - 0s 118us/step - loss: 1.9163\n",
            "Epoch 549/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 1.9158\n",
            "Epoch 550/1000\n",
            "52/52 [==============================] - 0s 112us/step - loss: 1.9153\n",
            "Epoch 551/1000\n",
            "52/52 [==============================] - 0s 114us/step - loss: 1.9147\n",
            "Epoch 552/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 1.9141\n",
            "Epoch 553/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 1.9136\n",
            "Epoch 554/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.9130\n",
            "Epoch 555/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.9125\n",
            "Epoch 556/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.9119\n",
            "Epoch 557/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 1.9114\n",
            "Epoch 558/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.9108\n",
            "Epoch 559/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.9103\n",
            "Epoch 560/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.9097\n",
            "Epoch 561/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 1.9092\n",
            "Epoch 562/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 1.9086\n",
            "Epoch 563/1000\n",
            "52/52 [==============================] - 0s 164us/step - loss: 1.9081\n",
            "Epoch 564/1000\n",
            "52/52 [==============================] - 0s 157us/step - loss: 1.9076\n",
            "Epoch 565/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.9070\n",
            "Epoch 566/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 1.9065\n",
            "Epoch 567/1000\n",
            "52/52 [==============================] - 0s 93us/step - loss: 1.9060\n",
            "Epoch 568/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.9054\n",
            "Epoch 569/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 1.9049\n",
            "Epoch 570/1000\n",
            "52/52 [==============================] - 0s 129us/step - loss: 1.9044\n",
            "Epoch 571/1000\n",
            "52/52 [==============================] - 0s 162us/step - loss: 1.9039\n",
            "Epoch 572/1000\n",
            "52/52 [==============================] - 0s 119us/step - loss: 1.9033\n",
            "Epoch 573/1000\n",
            "52/52 [==============================] - 0s 115us/step - loss: 1.9028\n",
            "Epoch 574/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.9022\n",
            "Epoch 575/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.9018\n",
            "Epoch 576/1000\n",
            "52/52 [==============================] - 0s 119us/step - loss: 1.9012\n",
            "Epoch 577/1000\n",
            "52/52 [==============================] - 0s 112us/step - loss: 1.9006\n",
            "Epoch 578/1000\n",
            "52/52 [==============================] - 0s 106us/step - loss: 1.9002\n",
            "Epoch 579/1000\n",
            "52/52 [==============================] - 0s 95us/step - loss: 1.8996\n",
            "Epoch 580/1000\n",
            "52/52 [==============================] - 0s 92us/step - loss: 1.8991\n",
            "Epoch 581/1000\n",
            "52/52 [==============================] - 0s 102us/step - loss: 1.8985\n",
            "Epoch 582/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.8980\n",
            "Epoch 583/1000\n",
            "52/52 [==============================] - 0s 104us/step - loss: 1.8975\n",
            "Epoch 584/1000\n",
            "52/52 [==============================] - 0s 95us/step - loss: 1.8969\n",
            "Epoch 585/1000\n",
            "52/52 [==============================] - 0s 93us/step - loss: 1.8965\n",
            "Epoch 586/1000\n",
            "52/52 [==============================] - 0s 105us/step - loss: 1.8960\n",
            "Epoch 587/1000\n",
            "52/52 [==============================] - 0s 99us/step - loss: 1.8955\n",
            "Epoch 588/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 1.8949\n",
            "Epoch 589/1000\n",
            "52/52 [==============================] - 0s 102us/step - loss: 1.8945\n",
            "Epoch 590/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.8939\n",
            "Epoch 591/1000\n",
            "52/52 [==============================] - 0s 102us/step - loss: 1.8934\n",
            "Epoch 592/1000\n",
            "52/52 [==============================] - 0s 114us/step - loss: 1.8929\n",
            "Epoch 593/1000\n",
            "52/52 [==============================] - 0s 110us/step - loss: 1.8924\n",
            "Epoch 594/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 1.8919\n",
            "Epoch 595/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 1.8914\n",
            "Epoch 596/1000\n",
            "52/52 [==============================] - 0s 98us/step - loss: 1.8909\n",
            "Epoch 597/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 1.8904\n",
            "Epoch 598/1000\n",
            "52/52 [==============================] - 0s 112us/step - loss: 1.8899\n",
            "Epoch 599/1000\n",
            "52/52 [==============================] - 0s 109us/step - loss: 1.8895\n",
            "Epoch 600/1000\n",
            "52/52 [==============================] - 0s 106us/step - loss: 1.8889\n",
            "Epoch 601/1000\n",
            "52/52 [==============================] - 0s 107us/step - loss: 1.8885\n",
            "Epoch 602/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.8880\n",
            "Epoch 603/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.8875\n",
            "Epoch 604/1000\n",
            "52/52 [==============================] - 0s 160us/step - loss: 1.8870\n",
            "Epoch 605/1000\n",
            "52/52 [==============================] - 0s 156us/step - loss: 1.8865\n",
            "Epoch 606/1000\n",
            "52/52 [==============================] - 0s 161us/step - loss: 1.8860\n",
            "Epoch 607/1000\n",
            "52/52 [==============================] - 0s 173us/step - loss: 1.8855\n",
            "Epoch 608/1000\n",
            "52/52 [==============================] - 0s 171us/step - loss: 1.8851\n",
            "Epoch 609/1000\n",
            "52/52 [==============================] - 0s 155us/step - loss: 1.8845\n",
            "Epoch 610/1000\n",
            "52/52 [==============================] - 0s 153us/step - loss: 1.8841\n",
            "Epoch 611/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.8836\n",
            "Epoch 612/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 1.8831\n",
            "Epoch 613/1000\n",
            "52/52 [==============================] - 0s 119us/step - loss: 1.8827\n",
            "Epoch 614/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 1.8822\n",
            "Epoch 615/1000\n",
            "52/52 [==============================] - 0s 232us/step - loss: 1.8816\n",
            "Epoch 616/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 1.8812\n",
            "Epoch 617/1000\n",
            "52/52 [==============================] - 0s 177us/step - loss: 1.8807\n",
            "Epoch 618/1000\n",
            "52/52 [==============================] - 0s 173us/step - loss: 1.8802\n",
            "Epoch 619/1000\n",
            "52/52 [==============================] - 0s 171us/step - loss: 1.8798\n",
            "Epoch 620/1000\n",
            "52/52 [==============================] - 0s 200us/step - loss: 1.8793\n",
            "Epoch 621/1000\n",
            "52/52 [==============================] - 0s 179us/step - loss: 1.8788\n",
            "Epoch 622/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 1.8784\n",
            "Epoch 623/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 1.8779\n",
            "Epoch 624/1000\n",
            "52/52 [==============================] - 0s 165us/step - loss: 1.8775\n",
            "Epoch 625/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 1.8769\n",
            "Epoch 626/1000\n",
            "52/52 [==============================] - 0s 186us/step - loss: 1.8765\n",
            "Epoch 627/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 1.8760\n",
            "Epoch 628/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 1.8755\n",
            "Epoch 629/1000\n",
            "52/52 [==============================] - 0s 175us/step - loss: 1.8750\n",
            "Epoch 630/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 1.8746\n",
            "Epoch 631/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 1.8741\n",
            "Epoch 632/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 1.8737\n",
            "Epoch 633/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.8732\n",
            "Epoch 634/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 1.8727\n",
            "Epoch 635/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.8723\n",
            "Epoch 636/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.8718\n",
            "Epoch 637/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 1.8714\n",
            "Epoch 638/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 1.8709\n",
            "Epoch 639/1000\n",
            "52/52 [==============================] - 0s 178us/step - loss: 1.8705\n",
            "Epoch 640/1000\n",
            "52/52 [==============================] - 0s 198us/step - loss: 1.8700\n",
            "Epoch 641/1000\n",
            "52/52 [==============================] - 0s 165us/step - loss: 1.8696\n",
            "Epoch 642/1000\n",
            "52/52 [==============================] - 0s 159us/step - loss: 1.8691\n",
            "Epoch 643/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 1.8687\n",
            "Epoch 644/1000\n",
            "52/52 [==============================] - 0s 156us/step - loss: 1.8682\n",
            "Epoch 645/1000\n",
            "52/52 [==============================] - 0s 156us/step - loss: 1.8678\n",
            "Epoch 646/1000\n",
            "52/52 [==============================] - 0s 159us/step - loss: 1.8673\n",
            "Epoch 647/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.8670\n",
            "Epoch 648/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.8664\n",
            "Epoch 649/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 1.8660\n",
            "Epoch 650/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 1.8655\n",
            "Epoch 651/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 1.8651\n",
            "Epoch 652/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 1.8647\n",
            "Epoch 653/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 1.8642\n",
            "Epoch 654/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 1.8638\n",
            "Epoch 655/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 1.8634\n",
            "Epoch 656/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 1.8629\n",
            "Epoch 657/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 1.8624\n",
            "Epoch 658/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 1.8620\n",
            "Epoch 659/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.8615\n",
            "Epoch 660/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 1.8612\n",
            "Epoch 661/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 1.8607\n",
            "Epoch 662/1000\n",
            "52/52 [==============================] - 0s 167us/step - loss: 1.8603\n",
            "Epoch 663/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 1.8598\n",
            "Epoch 664/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 1.8594\n",
            "Epoch 665/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.8590\n",
            "Epoch 666/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.8586\n",
            "Epoch 667/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 1.8581\n",
            "Epoch 668/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 1.8577\n",
            "Epoch 669/1000\n",
            "52/52 [==============================] - 0s 151us/step - loss: 1.8573\n",
            "Epoch 670/1000\n",
            "52/52 [==============================] - 0s 154us/step - loss: 1.8569\n",
            "Epoch 671/1000\n",
            "52/52 [==============================] - 0s 169us/step - loss: 1.8564\n",
            "Epoch 672/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.8560\n",
            "Epoch 673/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.8556\n",
            "Epoch 674/1000\n",
            "52/52 [==============================] - 0s 154us/step - loss: 1.8551\n",
            "Epoch 675/1000\n",
            "52/52 [==============================] - 0s 157us/step - loss: 1.8548\n",
            "Epoch 676/1000\n",
            "52/52 [==============================] - 0s 156us/step - loss: 1.8544\n",
            "Epoch 677/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.8539\n",
            "Epoch 678/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.8535\n",
            "Epoch 679/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.8531\n",
            "Epoch 680/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 1.8527\n",
            "Epoch 681/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 1.8522\n",
            "Epoch 682/1000\n",
            "52/52 [==============================] - 0s 110us/step - loss: 1.8519\n",
            "Epoch 683/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 1.8515\n",
            "Epoch 684/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 1.8511\n",
            "Epoch 685/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.8506\n",
            "Epoch 686/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.8502\n",
            "Epoch 687/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.8498\n",
            "Epoch 688/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 1.8495\n",
            "Epoch 689/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 1.8490\n",
            "Epoch 690/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.8486\n",
            "Epoch 691/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 1.8482\n",
            "Epoch 692/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 1.8478\n",
            "Epoch 693/1000\n",
            "52/52 [==============================] - 0s 118us/step - loss: 1.8474\n",
            "Epoch 694/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 1.8470\n",
            "Epoch 695/1000\n",
            "52/52 [==============================] - 0s 119us/step - loss: 1.8466\n",
            "Epoch 696/1000\n",
            "52/52 [==============================] - 0s 124us/step - loss: 1.8462\n",
            "Epoch 697/1000\n",
            "52/52 [==============================] - 0s 175us/step - loss: 1.8458\n",
            "Epoch 698/1000\n",
            "52/52 [==============================] - 0s 112us/step - loss: 1.8454\n",
            "Epoch 699/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 1.8450\n",
            "Epoch 700/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 1.8446\n",
            "Epoch 701/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.8442\n",
            "Epoch 702/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.8438\n",
            "Epoch 703/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.8434\n",
            "Epoch 704/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 1.8431\n",
            "Epoch 705/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.8427\n",
            "Epoch 706/1000\n",
            "52/52 [==============================] - 0s 147us/step - loss: 1.8423\n",
            "Epoch 707/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.8419\n",
            "Epoch 708/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 1.8415\n",
            "Epoch 709/1000\n",
            "52/52 [==============================] - 0s 158us/step - loss: 1.8412\n",
            "Epoch 710/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.8408\n",
            "Epoch 711/1000\n",
            "52/52 [==============================] - 0s 162us/step - loss: 1.8404\n",
            "Epoch 712/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.8400\n",
            "Epoch 713/1000\n",
            "52/52 [==============================] - 0s 293us/step - loss: 1.8396\n",
            "Epoch 714/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 1.8392\n",
            "Epoch 715/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.8389\n",
            "Epoch 716/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.8385\n",
            "Epoch 717/1000\n",
            "52/52 [==============================] - 0s 175us/step - loss: 1.8381\n",
            "Epoch 718/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 1.8377\n",
            "Epoch 719/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 1.8374\n",
            "Epoch 720/1000\n",
            "52/52 [==============================] - 0s 153us/step - loss: 1.8370\n",
            "Epoch 721/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 1.8366\n",
            "Epoch 722/1000\n",
            "52/52 [==============================] - 0s 100us/step - loss: 1.8363\n",
            "Epoch 723/1000\n",
            "52/52 [==============================] - 0s 119us/step - loss: 1.8359\n",
            "Epoch 724/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.8355\n",
            "Epoch 725/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 1.8352\n",
            "Epoch 726/1000\n",
            "52/52 [==============================] - 0s 151us/step - loss: 1.8348\n",
            "Epoch 727/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.8344\n",
            "Epoch 728/1000\n",
            "52/52 [==============================] - 0s 193us/step - loss: 1.8341\n",
            "Epoch 729/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.8337\n",
            "Epoch 730/1000\n",
            "52/52 [==============================] - 0s 163us/step - loss: 1.8333\n",
            "Epoch 731/1000\n",
            "52/52 [==============================] - 0s 153us/step - loss: 1.8329\n",
            "Epoch 732/1000\n",
            "52/52 [==============================] - 0s 147us/step - loss: 1.8326\n",
            "Epoch 733/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 1.8323\n",
            "Epoch 734/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 1.8318\n",
            "Epoch 735/1000\n",
            "52/52 [==============================] - 0s 103us/step - loss: 1.8315\n",
            "Epoch 736/1000\n",
            "52/52 [==============================] - 0s 184us/step - loss: 1.8311\n",
            "Epoch 737/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 1.8308\n",
            "Epoch 738/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 1.8304\n",
            "Epoch 739/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.8301\n",
            "Epoch 740/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.8297\n",
            "Epoch 741/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 1.8293\n",
            "Epoch 742/1000\n",
            "52/52 [==============================] - 0s 189us/step - loss: 1.8290\n",
            "Epoch 743/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 1.8287\n",
            "Epoch 744/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.8283\n",
            "Epoch 745/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.8279\n",
            "Epoch 746/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.8276\n",
            "Epoch 747/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.8273\n",
            "Epoch 748/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 1.8269\n",
            "Epoch 749/1000\n",
            "52/52 [==============================] - 0s 161us/step - loss: 1.8266\n",
            "Epoch 750/1000\n",
            "52/52 [==============================] - 0s 158us/step - loss: 1.8262\n",
            "Epoch 751/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 1.8259\n",
            "Epoch 752/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 1.8255\n",
            "Epoch 753/1000\n",
            "52/52 [==============================] - 0s 168us/step - loss: 1.8252\n",
            "Epoch 754/1000\n",
            "52/52 [==============================] - 0s 180us/step - loss: 1.8249\n",
            "Epoch 755/1000\n",
            "52/52 [==============================] - 0s 175us/step - loss: 1.8246\n",
            "Epoch 756/1000\n",
            "52/52 [==============================] - 0s 167us/step - loss: 1.8242\n",
            "Epoch 757/1000\n",
            "52/52 [==============================] - 0s 156us/step - loss: 1.8239\n",
            "Epoch 758/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 1.8236\n",
            "Epoch 759/1000\n",
            "52/52 [==============================] - 0s 116us/step - loss: 1.8232\n",
            "Epoch 760/1000\n",
            "52/52 [==============================] - 0s 160us/step - loss: 1.8229\n",
            "Epoch 761/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.8225\n",
            "Epoch 762/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.8222\n",
            "Epoch 763/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.8219\n",
            "Epoch 764/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.8215\n",
            "Epoch 765/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 1.8212\n",
            "Epoch 766/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.8209\n",
            "Epoch 767/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 1.8205\n",
            "Epoch 768/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 1.8202\n",
            "Epoch 769/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 1.8199\n",
            "Epoch 770/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 1.8195\n",
            "Epoch 771/1000\n",
            "52/52 [==============================] - 0s 115us/step - loss: 1.8192\n",
            "Epoch 772/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.8189\n",
            "Epoch 773/1000\n",
            "52/52 [==============================] - 0s 116us/step - loss: 1.8185\n",
            "Epoch 774/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.8182\n",
            "Epoch 775/1000\n",
            "52/52 [==============================] - 0s 147us/step - loss: 1.8179\n",
            "Epoch 776/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.8176\n",
            "Epoch 777/1000\n",
            "52/52 [==============================] - 0s 108us/step - loss: 1.8172\n",
            "Epoch 778/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 1.8169\n",
            "Epoch 779/1000\n",
            "52/52 [==============================] - 0s 124us/step - loss: 1.8166\n",
            "Epoch 780/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.8163\n",
            "Epoch 781/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 1.8159\n",
            "Epoch 782/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.8157\n",
            "Epoch 783/1000\n",
            "52/52 [==============================] - 0s 157us/step - loss: 1.8153\n",
            "Epoch 784/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.8150\n",
            "Epoch 785/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 1.8147\n",
            "Epoch 786/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 1.8144\n",
            "Epoch 787/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.8141\n",
            "Epoch 788/1000\n",
            "52/52 [==============================] - 0s 278us/step - loss: 1.8137\n",
            "Epoch 789/1000\n",
            "52/52 [==============================] - 0s 114us/step - loss: 1.8134\n",
            "Epoch 790/1000\n",
            "52/52 [==============================] - 0s 113us/step - loss: 1.8131\n",
            "Epoch 791/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.8128\n",
            "Epoch 792/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 1.8125\n",
            "Epoch 793/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 1.8122\n",
            "Epoch 794/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 1.8119\n",
            "Epoch 795/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 1.8116\n",
            "Epoch 796/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 1.8112\n",
            "Epoch 797/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.8110\n",
            "Epoch 798/1000\n",
            "52/52 [==============================] - 0s 147us/step - loss: 1.8106\n",
            "Epoch 799/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 1.8104\n",
            "Epoch 800/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.8101\n",
            "Epoch 801/1000\n",
            "52/52 [==============================] - 0s 110us/step - loss: 1.8098\n",
            "Epoch 802/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.8095\n",
            "Epoch 803/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 1.8091\n",
            "Epoch 804/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.8089\n",
            "Epoch 805/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 1.8086\n",
            "Epoch 806/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 1.8083\n",
            "Epoch 807/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.8080\n",
            "Epoch 808/1000\n",
            "52/52 [==============================] - 0s 124us/step - loss: 1.8077\n",
            "Epoch 809/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.8074\n",
            "Epoch 810/1000\n",
            "52/52 [==============================] - 0s 111us/step - loss: 1.8071\n",
            "Epoch 811/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.8068\n",
            "Epoch 812/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.8065\n",
            "Epoch 813/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 1.8062\n",
            "Epoch 814/1000\n",
            "52/52 [==============================] - 0s 124us/step - loss: 1.8060\n",
            "Epoch 815/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 1.8056\n",
            "Epoch 816/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 1.8054\n",
            "Epoch 817/1000\n",
            "52/52 [==============================] - 0s 105us/step - loss: 1.8051\n",
            "Epoch 818/1000\n",
            "52/52 [==============================] - 0s 118us/step - loss: 1.8048\n",
            "Epoch 819/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 1.8045\n",
            "Epoch 820/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 1.8041\n",
            "Epoch 821/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.8038\n",
            "Epoch 822/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 1.8036\n",
            "Epoch 823/1000\n",
            "52/52 [==============================] - 0s 118us/step - loss: 1.8033\n",
            "Epoch 824/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 1.8030\n",
            "Epoch 825/1000\n",
            "52/52 [==============================] - 0s 159us/step - loss: 1.8027\n",
            "Epoch 826/1000\n",
            "52/52 [==============================] - 0s 119us/step - loss: 1.8025\n",
            "Epoch 827/1000\n",
            "52/52 [==============================] - 0s 124us/step - loss: 1.8022\n",
            "Epoch 828/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 1.8019\n",
            "Epoch 829/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 1.8016\n",
            "Epoch 830/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.8013\n",
            "Epoch 831/1000\n",
            "52/52 [==============================] - 0s 171us/step - loss: 1.8010\n",
            "Epoch 832/1000\n",
            "52/52 [==============================] - 0s 168us/step - loss: 1.8008\n",
            "Epoch 833/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 1.8005\n",
            "Epoch 834/1000\n",
            "52/52 [==============================] - 0s 172us/step - loss: 1.8002\n",
            "Epoch 835/1000\n",
            "52/52 [==============================] - 0s 155us/step - loss: 1.7999\n",
            "Epoch 836/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.7997\n",
            "Epoch 837/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.7994\n",
            "Epoch 838/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 1.7991\n",
            "Epoch 839/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 1.7988\n",
            "Epoch 840/1000\n",
            "52/52 [==============================] - 0s 173us/step - loss: 1.7986\n",
            "Epoch 841/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 1.7983\n",
            "Epoch 842/1000\n",
            "52/52 [==============================] - 0s 151us/step - loss: 1.7980\n",
            "Epoch 843/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 1.7977\n",
            "Epoch 844/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 1.7974\n",
            "Epoch 845/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 1.7972\n",
            "Epoch 846/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 1.7969\n",
            "Epoch 847/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 1.7966\n",
            "Epoch 848/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 1.7963\n",
            "Epoch 849/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.7962\n",
            "Epoch 850/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.7958\n",
            "Epoch 851/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 1.7955\n",
            "Epoch 852/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.7953\n",
            "Epoch 853/1000\n",
            "52/52 [==============================] - 0s 157us/step - loss: 1.7950\n",
            "Epoch 854/1000\n",
            "52/52 [==============================] - 0s 153us/step - loss: 1.7947\n",
            "Epoch 855/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.7944\n",
            "Epoch 856/1000\n",
            "52/52 [==============================] - 0s 159us/step - loss: 1.7941\n",
            "Epoch 857/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 1.7939\n",
            "Epoch 858/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.7936\n",
            "Epoch 859/1000\n",
            "52/52 [==============================] - 0s 147us/step - loss: 1.7934\n",
            "Epoch 860/1000\n",
            "52/52 [==============================] - 0s 137us/step - loss: 1.7931\n",
            "Epoch 861/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.7928\n",
            "Epoch 862/1000\n",
            "52/52 [==============================] - 0s 174us/step - loss: 1.7926\n",
            "Epoch 863/1000\n",
            "52/52 [==============================] - 0s 166us/step - loss: 1.7923\n",
            "Epoch 864/1000\n",
            "52/52 [==============================] - 0s 153us/step - loss: 1.7921\n",
            "Epoch 865/1000\n",
            "52/52 [==============================] - 0s 153us/step - loss: 1.7917\n",
            "Epoch 866/1000\n",
            "52/52 [==============================] - 0s 152us/step - loss: 1.7915\n",
            "Epoch 867/1000\n",
            "52/52 [==============================] - 0s 152us/step - loss: 1.7913\n",
            "Epoch 868/1000\n",
            "52/52 [==============================] - 0s 174us/step - loss: 1.7910\n",
            "Epoch 869/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 1.7907\n",
            "Epoch 870/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 1.7905\n",
            "Epoch 871/1000\n",
            "52/52 [==============================] - 0s 167us/step - loss: 1.7902\n",
            "Epoch 872/1000\n",
            "52/52 [==============================] - 0s 159us/step - loss: 1.7899\n",
            "Epoch 873/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 1.7898\n",
            "Epoch 874/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 1.7895\n",
            "Epoch 875/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.7892\n",
            "Epoch 876/1000\n",
            "52/52 [==============================] - 0s 124us/step - loss: 1.7889\n",
            "Epoch 877/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 1.7887\n",
            "Epoch 878/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.7885\n",
            "Epoch 879/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.7882\n",
            "Epoch 880/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 1.7880\n",
            "Epoch 881/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 1.7877\n",
            "Epoch 882/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.7875\n",
            "Epoch 883/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 1.7873\n",
            "Epoch 884/1000\n",
            "52/52 [==============================] - 0s 125us/step - loss: 1.7870\n",
            "Epoch 885/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.7867\n",
            "Epoch 886/1000\n",
            "52/52 [==============================] - 0s 118us/step - loss: 1.7865\n",
            "Epoch 887/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.7862\n",
            "Epoch 888/1000\n",
            "52/52 [==============================] - 0s 159us/step - loss: 1.7860\n",
            "Epoch 889/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 1.7857\n",
            "Epoch 890/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.7856\n",
            "Epoch 891/1000\n",
            "52/52 [==============================] - 0s 151us/step - loss: 1.7853\n",
            "Epoch 892/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.7850\n",
            "Epoch 893/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 1.7848\n",
            "Epoch 894/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 1.7845\n",
            "Epoch 895/1000\n",
            "52/52 [==============================] - 0s 129us/step - loss: 1.7843\n",
            "Epoch 896/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.7841\n",
            "Epoch 897/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 1.7839\n",
            "Epoch 898/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 1.7836\n",
            "Epoch 899/1000\n",
            "52/52 [==============================] - 0s 124us/step - loss: 1.7834\n",
            "Epoch 900/1000\n",
            "52/52 [==============================] - 0s 129us/step - loss: 1.7832\n",
            "Epoch 901/1000\n",
            "52/52 [==============================] - 0s 154us/step - loss: 1.7829\n",
            "Epoch 902/1000\n",
            "52/52 [==============================] - 0s 151us/step - loss: 1.7826\n",
            "Epoch 903/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 1.7825\n",
            "Epoch 904/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.7822\n",
            "Epoch 905/1000\n",
            "52/52 [==============================] - 0s 112us/step - loss: 1.7820\n",
            "Epoch 906/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 1.7817\n",
            "Epoch 907/1000\n",
            "52/52 [==============================] - 0s 114us/step - loss: 1.7815\n",
            "Epoch 908/1000\n",
            "52/52 [==============================] - 0s 93us/step - loss: 1.7813\n",
            "Epoch 909/1000\n",
            "52/52 [==============================] - 0s 113us/step - loss: 1.7811\n",
            "Epoch 910/1000\n",
            "52/52 [==============================] - 0s 88us/step - loss: 1.7808\n",
            "Epoch 911/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 1.7806\n",
            "Epoch 912/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.7804\n",
            "Epoch 913/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.7801\n",
            "Epoch 914/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 1.7799\n",
            "Epoch 915/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 1.7797\n",
            "Epoch 916/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 1.7794\n",
            "Epoch 917/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.7792\n",
            "Epoch 918/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.7790\n",
            "Epoch 919/1000\n",
            "52/52 [==============================] - 0s 136us/step - loss: 1.7788\n",
            "Epoch 920/1000\n",
            "52/52 [==============================] - 0s 174us/step - loss: 1.7785\n",
            "Epoch 921/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.7783\n",
            "Epoch 922/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 1.7781\n",
            "Epoch 923/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.7779\n",
            "Epoch 924/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 1.7777\n",
            "Epoch 925/1000\n",
            "52/52 [==============================] - 0s 171us/step - loss: 1.7774\n",
            "Epoch 926/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.7772\n",
            "Epoch 927/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 1.7769\n",
            "Epoch 928/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 1.7768\n",
            "Epoch 929/1000\n",
            "52/52 [==============================] - 0s 134us/step - loss: 1.7765\n",
            "Epoch 930/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 1.7763\n",
            "Epoch 931/1000\n",
            "52/52 [==============================] - 0s 129us/step - loss: 1.7761\n",
            "Epoch 932/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 1.7759\n",
            "Epoch 933/1000\n",
            "52/52 [==============================] - 0s 121us/step - loss: 1.7757\n",
            "Epoch 934/1000\n",
            "52/52 [==============================] - 0s 143us/step - loss: 1.7754\n",
            "Epoch 935/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.7752\n",
            "Epoch 936/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.7750\n",
            "Epoch 937/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 1.7748\n",
            "Epoch 938/1000\n",
            "52/52 [==============================] - 0s 117us/step - loss: 1.7745\n",
            "Epoch 939/1000\n",
            "52/52 [==============================] - 0s 171us/step - loss: 1.7743\n",
            "Epoch 940/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 1.7741\n",
            "Epoch 941/1000\n",
            "52/52 [==============================] - 0s 146us/step - loss: 1.7739\n",
            "Epoch 942/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 1.7737\n",
            "Epoch 943/1000\n",
            "52/52 [==============================] - 0s 138us/step - loss: 1.7735\n",
            "Epoch 944/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.7733\n",
            "Epoch 945/1000\n",
            "52/52 [==============================] - 0s 162us/step - loss: 1.7731\n",
            "Epoch 946/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 1.7728\n",
            "Epoch 947/1000\n",
            "52/52 [==============================] - 0s 154us/step - loss: 1.7726\n",
            "Epoch 948/1000\n",
            "52/52 [==============================] - 0s 161us/step - loss: 1.7724\n",
            "Epoch 949/1000\n",
            "52/52 [==============================] - 0s 152us/step - loss: 1.7722\n",
            "Epoch 950/1000\n",
            "52/52 [==============================] - 0s 153us/step - loss: 1.7720\n",
            "Epoch 951/1000\n",
            "52/52 [==============================] - 0s 148us/step - loss: 1.7718\n",
            "Epoch 952/1000\n",
            "52/52 [==============================] - 0s 287us/step - loss: 1.7716\n",
            "Epoch 953/1000\n",
            "52/52 [==============================] - 0s 150us/step - loss: 1.7714\n",
            "Epoch 954/1000\n",
            "52/52 [==============================] - 0s 184us/step - loss: 1.7712\n",
            "Epoch 955/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 1.7710\n",
            "Epoch 956/1000\n",
            "52/52 [==============================] - 0s 127us/step - loss: 1.7708\n",
            "Epoch 957/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 1.7706\n",
            "Epoch 958/1000\n",
            "52/52 [==============================] - 0s 129us/step - loss: 1.7704\n",
            "Epoch 959/1000\n",
            "52/52 [==============================] - 0s 179us/step - loss: 1.7702\n",
            "Epoch 960/1000\n",
            "52/52 [==============================] - 0s 135us/step - loss: 1.7700\n",
            "Epoch 961/1000\n",
            "52/52 [==============================] - 0s 114us/step - loss: 1.7698\n",
            "Epoch 962/1000\n",
            "52/52 [==============================] - 0s 124us/step - loss: 1.7695\n",
            "Epoch 963/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 1.7694\n",
            "Epoch 964/1000\n",
            "52/52 [==============================] - 0s 116us/step - loss: 1.7692\n",
            "Epoch 965/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.7690\n",
            "Epoch 966/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.7687\n",
            "Epoch 967/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.7686\n",
            "Epoch 968/1000\n",
            "52/52 [==============================] - 0s 123us/step - loss: 1.7684\n",
            "Epoch 969/1000\n",
            "52/52 [==============================] - 0s 120us/step - loss: 1.7682\n",
            "Epoch 970/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.7680\n",
            "Epoch 971/1000\n",
            "52/52 [==============================] - 0s 133us/step - loss: 1.7678\n",
            "Epoch 972/1000\n",
            "52/52 [==============================] - 0s 113us/step - loss: 1.7676\n",
            "Epoch 973/1000\n",
            "52/52 [==============================] - 0s 129us/step - loss: 1.7675\n",
            "Epoch 974/1000\n",
            "52/52 [==============================] - 0s 132us/step - loss: 1.7672\n",
            "Epoch 975/1000\n",
            "52/52 [==============================] - 0s 153us/step - loss: 1.7670\n",
            "Epoch 976/1000\n",
            "52/52 [==============================] - 0s 154us/step - loss: 1.7668\n",
            "Epoch 977/1000\n",
            "52/52 [==============================] - 0s 139us/step - loss: 1.7666\n",
            "Epoch 978/1000\n",
            "52/52 [==============================] - 0s 140us/step - loss: 1.7665\n",
            "Epoch 979/1000\n",
            "52/52 [==============================] - 0s 130us/step - loss: 1.7663\n",
            "Epoch 980/1000\n",
            "52/52 [==============================] - 0s 106us/step - loss: 1.7661\n",
            "Epoch 981/1000\n",
            "52/52 [==============================] - 0s 156us/step - loss: 1.7659\n",
            "Epoch 982/1000\n",
            "52/52 [==============================] - 0s 141us/step - loss: 1.7657\n",
            "Epoch 983/1000\n",
            "52/52 [==============================] - 0s 131us/step - loss: 1.7655\n",
            "Epoch 984/1000\n",
            "52/52 [==============================] - 0s 142us/step - loss: 1.7653\n",
            "Epoch 985/1000\n",
            "52/52 [==============================] - 0s 128us/step - loss: 1.7651\n",
            "Epoch 986/1000\n",
            "52/52 [==============================] - 0s 149us/step - loss: 1.7650\n",
            "Epoch 987/1000\n",
            "52/52 [==============================] - 0s 177us/step - loss: 1.7648\n",
            "Epoch 988/1000\n",
            "52/52 [==============================] - 0s 155us/step - loss: 1.7645\n",
            "Epoch 989/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 1.7644\n",
            "Epoch 990/1000\n",
            "52/52 [==============================] - 0s 122us/step - loss: 1.7642\n",
            "Epoch 991/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 1.7640\n",
            "Epoch 992/1000\n",
            "52/52 [==============================] - 0s 163us/step - loss: 1.7638\n",
            "Epoch 993/1000\n",
            "52/52 [==============================] - 0s 144us/step - loss: 1.7637\n",
            "Epoch 994/1000\n",
            "52/52 [==============================] - 0s 126us/step - loss: 1.7635\n",
            "Epoch 995/1000\n",
            "52/52 [==============================] - 0s 113us/step - loss: 1.7633\n",
            "Epoch 996/1000\n",
            "52/52 [==============================] - 0s 145us/step - loss: 1.7631\n",
            "Epoch 997/1000\n",
            "52/52 [==============================] - 0s 109us/step - loss: 1.7629\n",
            "Epoch 998/1000\n",
            "52/52 [==============================] - 0s 98us/step - loss: 1.7628\n",
            "Epoch 999/1000\n",
            "52/52 [==============================] - 0s 107us/step - loss: 1.7626\n",
            "Epoch 1000/1000\n",
            "52/52 [==============================] - 0s 124us/step - loss: 1.7624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f307308af28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7py2Oe98F0l_",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMbKXrBZ_AZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "03bc49b7-cd4e-4289-bc6e-5a322f74f8b4"
      },
      "source": [
        "model.layers[0].get_weights()[0]"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.7867562 ,  1.3119941 ],\n",
              "       [-1.0833639 , -0.6052263 ],\n",
              "       [-0.91311455, -1.6020463 ],\n",
              "       [-1.4275825 ,  0.4532969 ],\n",
              "       [ 1.0783024 , -0.6757577 ],\n",
              "       [ 0.9572486 ,  0.80008644],\n",
              "       [ 0.8106273 ,  0.34368762],\n",
              "       [-0.9619231 , -0.29547948],\n",
              "       [-0.8773608 , -1.0685834 ],\n",
              "       [ 1.1984353 , -0.6700955 ],\n",
              "       [ 0.47249597,  0.42572063],\n",
              "       [-0.19824633,  0.3284137 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPSarQTmIMkM",
        "colab_type": "code",
        "outputId": "c6849ecc-00fc-44bc-aa1c-0d71fba8a3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "word2int"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boy': 7,\n",
              " 'girl': 10,\n",
              " 'king': 1,\n",
              " 'man': 2,\n",
              " 'pretty': 0,\n",
              " 'prince': 8,\n",
              " 'princess': 4,\n",
              " 'queen': 6,\n",
              " 'strong': 3,\n",
              " 'wise': 5,\n",
              " 'woman': 9,\n",
              " 'young': 11}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70dgj35mIN33",
        "colab_type": "code",
        "outputId": "3f969068-7336-45f1-b609-f3e618ab0c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "int2word = {v: k for k, v in word2int.items()} ; int2word"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'pretty',\n",
              " 1: 'king',\n",
              " 2: 'man',\n",
              " 3: 'strong',\n",
              " 4: 'princess',\n",
              " 5: 'wise',\n",
              " 6: 'queen',\n",
              " 7: 'boy',\n",
              " 8: 'prince',\n",
              " 9: 'woman',\n",
              " 10: 'girl',\n",
              " 11: 'young'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n3VjEGJDp5N",
        "colab_type": "code",
        "outputId": "cd349b6b-79bc-456a-eec3-aebba53150a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "e_weights = model.layers[0].get_weights()[0]; e_weights"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.7867562 ,  1.3119941 ],\n",
              "       [-1.0833639 , -0.6052263 ],\n",
              "       [-0.91311455, -1.6020463 ],\n",
              "       [-1.4275825 ,  0.4532969 ],\n",
              "       [ 1.0783024 , -0.6757577 ],\n",
              "       [ 0.9572486 ,  0.80008644],\n",
              "       [ 0.8106273 ,  0.34368762],\n",
              "       [-0.9619231 , -0.29547948],\n",
              "       [-0.8773608 , -1.0685834 ],\n",
              "       [ 1.1984353 , -0.6700955 ],\n",
              "       [ 0.47249597,  0.42572063],\n",
              "       [-0.19824633,  0.3284137 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB5iKSGIE1l0",
        "colab_type": "code",
        "outputId": "3ee0737d-2cf8-469b-f15e-f50f2e001df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(e_weights[:, 0], e_weights[:, 1])\n",
        "for i, coord in enumerate(e_weights):\n",
        "    plt.text(coord[0], coord[1]+0., int2word[i])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAI/CAYAAAA2mq62AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4AGiDAAAwANEGQAAgAf+P2m+pK48C3OeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqHuuwxXH9Xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}